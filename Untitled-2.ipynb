{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e83bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports, Library Setup and  Dataset Paths.\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, f1_score,\n",
    "    matthews_corrcoef, precision_score, recall_score\n",
    ")\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "NORMAL_FOLDER = r\"G:\\Image Clssification\\Dataset_new\\normal\"\n",
    "TUMOR_FOLDER = r\"G:\\Image Clssification\\Dataset_new\\pancreatic_tumor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05980a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE LOADING & PREPROCESSING \n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "def load_images(folder, label):\n",
    "    data, labels = [], []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img = clahe.apply(img)\n",
    "            data.append(img)\n",
    "            labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Denoising & Contour Processing\n",
    "\n",
    "# Morphological Denoising\n",
    "def morph_denoise(img):\n",
    "    return cv2.morphologyEx(img, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "\n",
    "# Boundary tracing + FCC\n",
    "def boundary_following(binary):\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    return contours\n",
    "\n",
    "def freeman_chain_code(contour):\n",
    "    direction = []\n",
    "    for i in range(1, len(contour)):\n",
    "        dx = contour[i][0][0] - contour[i - 1][0][0]\n",
    "        dy = contour[i][0][1] - contour[i - 1][0][1]\n",
    "        code = {(1, 0): 0, (1, -1): 1, (0, -1): 2, (-1, -1): 3,\n",
    "                (-1, 0): 4, (-1, 1): 5, (0, 1): 6, (1, 1): 7}\n",
    "        direction.append(code.get((dx, dy), -1))\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a935f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE EXTRACTION ===\n",
    "def extract_shape_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        _, binary = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
    "        denoised = morph_denoise(binary)\n",
    "        contours = boundary_following(denoised)\n",
    "        if contours:\n",
    "            c = max(contours, key=cv2.contourArea)\n",
    "            area = cv2.contourArea(c)\n",
    "            perimeter = cv2.arcLength(c, True)\n",
    "            fcc = freeman_chain_code(c)\n",
    "            features.append([area, perimeter, len(fcc)])\n",
    "        else:\n",
    "            features.append([0, 0, 0])\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_glcm_features(images):\n",
    "    glcm_props = ['contrast', 'homogeneity', 'energy']\n",
    "    features = []\n",
    "    for img in images:\n",
    "        glcm = graycomatrix(img, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "        features.append([graycoprops(glcm, p)[0, 0] for p in glcm_props])\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_wavelet_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        cA, (cH, cV, cD) = pywt.wavedec2(img, 'sym4', level=2)[0:2]\n",
    "        features.append([\n",
    "            np.mean(cA), np.var(cA),\n",
    "            np.mean(cH), np.var(cH),\n",
    "            np.mean(cV), np.var(cV),\n",
    "            np.mean(cD), np.var(cD)\n",
    "        ])\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING ===\n",
    "normal_imgs, normal_labels = load_images(NORMAL_FOLDER, 0)\n",
    "tumor_imgs, tumor_labels = load_images(TUMOR_FOLDER, 1)\n",
    "images = np.vstack((normal_imgs, tumor_imgs))\n",
    "labels = np.hstack((normal_labels, tumor_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0659c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets: - Loads normal and tumor images with their labels\n",
    "normal_images, normal_labels = load_images(NORMAL_FOLDER, 0)\n",
    "tumor_images, tumor_labels = load_images(TUMOR_FOLDER, 1)\n",
    "images = np.vstack((normal_images, tumor_images))\n",
    "labels = np.hstack((normal_labels, tumor_labels))\n",
    "\n",
    "# Feature Extraction\n",
    "shape_features = extract_shape_features(images)  #shape - area, perimeter, chain code length\n",
    "glcm_features = extract_glcm_features(images)    #Texture -  GLCM (contrast, homogeneity, energy)\n",
    "wavelet_features = extract_wavelet_features(images)   #Frequency- Wavelet (mean & variance of subbands)\n",
    "\n",
    "features = np.hstack((shape_features, glcm_features, wavelet_features))\n",
    "np.save(\"features.npy\", features)       #using np Merges all features into one array: features.npy\n",
    "np.save(\"labels.npy\", labels)           #using np saves labels into labels.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5440a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPLIT BEFORE FEATURE EXTRACTION (TO PREVENT LEAKAGE) ===\n",
    "X_train_imgs, X_test_imgs, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.26, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE EXTRACTION ===\n",
    "def extract_features(images):\n",
    "    return np.hstack((\n",
    "        extract_shape_features(images),\n",
    "        extract_glcm_features(images),\n",
    "        extract_wavelet_features(images)\n",
    "    ))\n",
    "\n",
    "X_train = extract_features(X_train_imgs)\n",
    "X_test = extract_features(X_test_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RESAMPLING (SMOTE) ===\n",
    "X_train, y_train = SMOTE(sampling_strategy=0.7, random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "# === SCALING & PCA ===\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "pca = PCA(n_components=10).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "joblib.dump(pca, \"pca_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c49b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SVM HYPERPARAMETER SEARCH ===\n",
    "svm = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n",
    "param_dist = {'C': [1, 3, 5, 7, 10], 'gamma': ['scale', 0.01, 0.1, 1, 10]}\n",
    "random_search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=8, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a34d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL MODEL ===\n",
    "best_svm = SVC(kernel='rbf', probability=True, class_weight='balanced', C=best_params['C'], gamma=best_params['gamma'], random_state=42)\n",
    "bagged_svm = BaggingClassifier(estimator=best_svm, n_estimators=3, n_jobs=-1, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('svm', bagged_svm), ('xgb', xgb)], voting='soft', n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ===\n",
    "start_train = time.time()\n",
    "ensemble.fit(X_train, y_train)\n",
    "train_time = time.time() - start_train\n",
    "joblib.dump(ensemble, \"ensemble_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b832e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TESTING & METRICS ===\n",
    "start_test = time.time()\n",
    "y_train_pred = ensemble.predict(X_train)\n",
    "y_test_pred = ensemble.predict(X_test)\n",
    "y_test_prob = ensemble.predict_proba(X_test)[:, 1]\n",
    "test_time = time.time() - start_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d002297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METRICS ===\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "auc = roc_auc_score(y_test, y_test_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OUTPUT ===\n",
    "print(f\"Training Time: {train_time:.2f}s | Testing Time: {test_time:.2f}s\")\n",
    "print(f\"Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}%\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity*100:.2f}% | Specificity: {specificity*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}% | F1 Score: {f1:.2f}\")\n",
    "print(f\"MCC: {mcc:.2f} | AUC: {auc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
